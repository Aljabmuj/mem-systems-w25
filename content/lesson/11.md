+++
title = "Distributed Shared Memory"
[extra]
index = 11
thread = 48

[[extra.readings]]
name = "* Memory coherence in shared virtual memory systems"
url = "https://dl.acm.org/doi/pdf/10.1145/75104.75105"
details = "Kai Li and Paul Hudak, PODC '86"

[[extra.readings]]
name = "* Tread Marks: Distributed Shared Memory on Standard Workstations and Operating Systems"
url = "https://www.usenix.org/conference/usenix-winter-1994-technical-conference/tread-marks-distributed-shared-memory-standard"
details = "Keleher et al., USENIX Winter TC '94"

[[extra.readings]]
name = "* Latency-Tolerant Software Distributed Shared Memory"
url = "https://www.usenix.org/conference/atc15/technical-session/presentation/nelson"
details = "Nelson et al., USENIX ATC '15"

[[extra.readings]]
name = "* Concordia: Distributed Shared Memory with In-Network Cache Coherence"
url = "https://www.usenix.org/conference/fast21/presentation/wang"
details = "Wang et al., FAST '21"

[[extra.readings]]
name = "* GiantVM: A Novel Distributed Hypervisor for Resource Aggregation with DSM-aware Optimizations"
url = "https://dl.acm.org/doi/10.1145/3505251"
details = "Jia et al., ACM TACO '22"



+++

On readings:
Recommended background readings are marked with (^) above. Optional historical or fun readings are marked with (*). 
If you feel comfortable with the topic already, you may skip these readings. 

## Notes

### Page granularity sharing
DSM systems typically share data at the level of pages, contrasted with the cache line size. This is because we need to
amortize the (relatively) large costs of communicating over a loosely coupled newtork. These costs suggest the use of
large pages, but there is a problem...

### False sharing
We've talked about this before, but any time we share data in _chunks_, there is a possibility of two threads/processes accessing
distinct data in separate parts of the chunk. In a system that has to enforce coherence, such _false sharing_ confounds
the coherence protocol, generating unnecessary traffic on the interconnect. 

### Restricting shared memory
Note that in IVY, not _all_ memory can be shared across nodes. Some portion of the address space (in particular, the low portion), is
kept local, ensuring fast access. For example, the executable of processes is kept in local memory. However, the stack is not. The PCB
is kept private. 


### Note on terminology
The authors use the term _eventcount_ to describe a synchronization primitive. You will hopefully recognize this as what
we'd today call a _counting semaphore_. 

